# Train BLIPFeatureFusion model on MBEIR dataset
# <-- Important! Change this for each experiment.
experiment:
    instruct_status: "Instruct"
    exp_name: "InBatch"
    description: "${model.name} ${model.size} ${experiment.instruct_status} ${experiment.exp_name}"
    path_suffix: "${model.short_name}/${model.size}/${experiment.instruct_status}/${experiment.exp_name}/"

# WandB settings
wandb_config:
    enabled: True
    experiment_name: "${experiment.description}"

# Logger settings
logger_config:
    logger_out_dir: "logger/${experiment.path_suffix}"  # logger will be saved to uniir_dir/logger/experiment.path_suffix
    logger_out_file_name: "train.log"  #TODO: add date to log file name

# Dataset settings
data_config:
    image_size: 224, 224
    hard_neg_num: 0
    in_batch_neg_num: 0  # TODO: MOVE THIS TO MODEL Config
    shuffle_cand: True
    
    returns:
      hashed_p_did: True

    # Relative to mbeir_data_dir
    enable_query_instruct: True
    query_instruct_path: "instructions/query_instructions.tsv"

    train_query_data_path: "query/union_train/mbeir_union_up_train.jsonl"
    train_cand_pool_path: "cand_pool/global/mbeir_union_train_cand_pool.jsonl"

    val_query_data_path: "query/union_val/mbeir_union_val.jsonl"
    val_cand_pool_path: "cand_pool/global/mbeir_union_val_cand_pool.jsonl"

# DataLoader settings
dataloader_config:
    num_workers: 5
    train_batch_size: 115   # 79051MiB / 81559MiB
    valid_batch_size: 1035  # 57960/8/9

# Trainer settings
trainer_config:
    gradient_accumulation_steps: 1
    num_train_epochs: 20
    init_lr: 1e-5
    print_freq: 50
    weight_decay: 0.05

# Evaluator settings
evaluator:
    enable_eval: False
    eval_freq: 1
    print_freq: 10

# Model settings
model:
    name: "BLIPFeatureFusion"
    short_name: "BLIP_FF"
    size: "Large"

    vit: 'large'
    vit_grad_ckpt: True
    vit_ckpt_layer: 12  # Default

    embed_dim: 768
    image_size: 224
    queue_size: 57960 # <-- Important! You need to adjust this value based on your num_gpus and batchsize
    alpha: 0.4
    tokenizer_max_length: 100

    ckpt_config:
        ckpt_dir: "checkpoint/${experiment.path_suffix}" # ckpt will be saved to uniir_dir/checkpoint/experiment.path_suffix
        pretrained_blip_url: "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth"
        ckpt_name: ""
        resume_training: False


# Random seed
seed: 2023

# Distributed training settings
dist_config:
    dist_url: "env://"